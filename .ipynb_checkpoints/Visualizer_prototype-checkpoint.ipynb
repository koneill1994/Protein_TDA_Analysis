{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import math\n",
    "import imageio\n",
    "import json\n",
    "import time\n",
    "\n",
    "from collections import Counter\n",
    "from matplotlib.pyplot import figure\n",
    "from matplotlib import animation\n",
    "\n",
    "from Bio.PDB import *\n",
    "import nglview as nv\n",
    "import math\n",
    "import warnings\n",
    "from Bio.PDB.StructureBuilder import PDBConstructionWarning\n",
    "warnings.filterwarnings(\"ignore\", category=PDBConstructionWarning)\n",
    "\n",
    "# this is dangerous of course but uncomment it when you want to run the code\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "\n",
    "\n",
    "\n",
    "# seg_dir = \"C:\\Users\\Kevin\\Documents\\TDA\\afrl\\Segmentations\"\n",
    "# seg_dir = \"C:/Users/Kevin/Documents/TDA/afrl/Segmentations\"\n",
    "seg_dir = \"../Batch_protein_all_5_3_no_na.csv_2019.07.08 16.10/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aaron's code to make sure the folders are all in their proper place\n",
    "\n",
    "dirlocal = os.path.curdir\n",
    "data_path = os.path.join(dirlocal, 'Data/')\n",
    "protein_path = os.path.join(data_path, 'Proteins/')\n",
    "pdb_path = os.path.join(data_path, 'PDB/')\n",
    "fasta_path = os.path.join(data_path, 'fasta/')\n",
    "dssp_path = os.path.join(data_path, 'DSSP/')\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "if not os.path.exists(protein_path):\n",
    "    os.makedirs(protein_path)\n",
    "if not os.path.exists(pdb_path):\n",
    "    os.makedirs(pdb_path)\n",
    "ss_path = os.path.join(data_path, 'ss.txt')\n",
    "feature_path = os.path.join(data_path, 'sample-input-features.npy')\n",
    "distance_path = os.path.join(data_path, 'sample-distance-maps-cb.npy')\n",
    "full_feature_path = os.path.join(data_path, 'full-input-features.npy')\n",
    "full_distance_path = os.path.join(data_path, 'full-distance-maps-cb.npy')\n",
    "test_feature_path = os.path.join(data_path, 'testset-input-features.npy')\n",
    "test_distance_path = os.path.join(data_path, 'testset-distance-maps-cb.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## aaron's code to get raw features from pdb\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import figure\n",
    "%matplotlib inline\n",
    "import time\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# ! pip install biopython\n",
    "# ! pip install nglview\n",
    "# ! jupyter-nbextension enable nglview --py --sys-prefix\n",
    "\n",
    "from Bio.PDB import *\n",
    "import nglview as nv\n",
    "import math\n",
    "import warnings\n",
    "from Bio.PDB.StructureBuilder import PDBConstructionWarning\n",
    "warnings.filterwarnings(\"ignore\", category=PDBConstructionWarning)\n",
    "\n",
    "# this is dangerous of course but uncomment it when you want to run the code\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_chip_start_stop(df, step_size, chip_size):\n",
    "\n",
    "    ch=[int(x.split(\"_\")[-1]) for x in df['chip_id']]\n",
    "\n",
    "    start=[c*step_size for c in ch]\n",
    "    stop=[chip_size+c*step_size for c in ch]\n",
    "\n",
    "    df['start']=start\n",
    "    df['stop']=stop\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  aaron's functions: \n",
    "\n",
    "# gets dataframe containing torsion angles, peptides, chain, etc\n",
    "# from pdb file\n",
    "def get_torsion_angles(pdb_id, degrees=False):\n",
    "    pdb_struct = get_pdb_structure(pdb_id)\n",
    "    torsion_angles = []\n",
    "    for model in pdb_struct:\n",
    "        for chain in model:\n",
    "            polypeptides = PPBuilder().build_peptides(chain)\n",
    "            for poly_index, poly in enumerate(polypeptides):\n",
    "                #print(\"Model %s Chain %s\" % (str(model.id), str(chain.id)))\n",
    "                #print(\"(part %i of %i)\" % (poly_index+1, len(polypeptides)))\n",
    "                #print(\"length %i\" % (len(poly)))\n",
    "                #print(\"from %s%i\" % (poly[0].resname, poly[0].id[1]))\n",
    "                #print(\"to %s%i\" % (poly[-1].resname, poly[-1].id[1]))\n",
    "                phi_psi = poly.get_phi_psi_list()\n",
    "                for res_index, residue in enumerate(poly):\n",
    "                    res_name = \"%s%i\" % (residue.resname, residue.id[1])\n",
    "                    #print(res_name, tuple(math.degrees(b) for b in phi_psi[res_index] if b))\n",
    "                    deg = phi_psi[res_index]\n",
    "                    if degrees:\n",
    "                        deg = tuple(math.degrees(b) if b else None for b in deg)\n",
    "                    phi, psi = deg\n",
    "                    model_name, model_id = model.full_id\n",
    "                    torsion_angles.append([model_name, model_id, chain.id, residue.resname, residue.id[1], phi, psi])\n",
    "    return pd.DataFrame(torsion_angles, columns=['Model_Name', 'Model_ID', 'Chain', 'Residue_Name', 'Residue_ID', 'Phi', 'Psi'])\n",
    "\n",
    "\n",
    "# generates ramachandran plot from a given pdb file\n",
    "def ramachandran_plot(pdb_id, degrees=True):\n",
    "    df = get_torsion_angles(pdb_id, degrees)\n",
    "    x = df['Phi']\n",
    "    y = df['Psi']\n",
    "    # Generate plot\n",
    "    plt.plot(x, y, \".\")\n",
    "    plt.title('Ramachandran Plot')\n",
    "    if degrees:\n",
    "        plt.xlabel(f'$\\Phi$ Angle (Degrees)')\n",
    "        plt.xlim(-180, 180)\n",
    "        plt.ylabel(f'$\\Psi$ Angle (Radians)')\n",
    "        plt.ylim(-180, 180)\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.xlabel(f'$\\Phi$ Angle (Radians)')\n",
    "        plt.xlim(-math.pi, math.pi)\n",
    "        plt.ylabel(f'$\\Psi$ Angle (Radians)')\n",
    "        plt.ylim(-math.pi, math.pi)\n",
    "        plt.show()\n",
    "\n",
    "# helper function which generates a biopython structure\n",
    "# for display in display_protein()\n",
    "def get_pdb_structure(pdb_id):\n",
    "    pdb_id = pdb_id.upper()\n",
    "    parser = PDBParser()\n",
    "    file_path = os.path.join(pdb_path, f\"{pdb_id}.pdb\")\n",
    "    try:\n",
    "        struct = parser.get_structure(pdb_id, file_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    return struct\n",
    "\n",
    "\n",
    "#  downloads the protein in question from pdb\n",
    "def get_pdb_file(pdb_id, replace=False):\n",
    "    pdb_id = pdb_id.upper()\n",
    "    file_path = os.path.join(pdb_path, f'{pdb_id}.pdb')\n",
    "    if os.path.isfile(file_path) and not replace:\n",
    "        return True\n",
    "    parser = PDBParser()\n",
    "    url = f'https://files.rcsb.org/download/{pdb_id}.pdb'\n",
    "    resp = requests.get(url)\n",
    "    try:\n",
    "        #print(file_path)\n",
    "        file = open(file_path, \"wb\")\n",
    "        file.write(resp.content)\n",
    "        file.close()\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# neato 3d viewer of a protein's structure\n",
    "def display_protein(pdb_id):\n",
    "    pdb_struct = get_pdb_structure(pdb_id)\n",
    "    view = nv.show_biopython(pdb_struct)\n",
    "    return view\n",
    "\n",
    "\n",
    "#  returns a dataframe with secondary structure, amino acid code, chain id, etc\n",
    "# deprecated\n",
    "def get_secondary_structure(pdb_id):\n",
    "    pdb_id = pdb_id.upper()\n",
    "    indexes = get_pdb_ss_seq(pdb_id)\n",
    "    chain_df = []\n",
    "    for i, chain_ss in enumerate(indexes):\n",
    "        chain_id, seq, ss = chain_ss\n",
    "        #unknown_list = [i for i, c in enumerate(seq) if c == 'X']\n",
    "        #print(unknown_list)\n",
    "        seq_list = [c for c in seq]\n",
    "        # [f(x) if condition else g(x) for x in sequence]\n",
    "        ss_list = ['L' if c is ' ' else c for c in ss]\n",
    "        #print(seq_list + ss_list)\n",
    "        chain_df.append(pd.DataFrame({'Chain': chain_id, 'Amino_Acid':seq_list, 'Secondary_Structure':ss_list}))\n",
    "    return pd.concat(chain_df)\n",
    "\n",
    "# get matrix of pairwise carbon beta distances\n",
    "def get_cb_distances(full_pdb_id, partial=False):\n",
    "    pdb_id = full_pdb_id[0:4].upper()\n",
    "    get_pdb_file(pdb_id)\n",
    "    pdb_struct = get_pdb_structure(pdb_id)  # Returns the BioPython structure of the PDB file\n",
    "    ppb = PPBuilder()\n",
    "    chain_dist_matrix = {}\n",
    "    for model in pdb_struct:\n",
    "        for chain in model:\n",
    "            if chain.id == full_pdb_id[4] and partial:\n",
    "                residues = []\n",
    "                for pp in ppb.build_peptides(chain, aa_only=False):\n",
    "                    for residue in pp:\n",
    "                        residues.append(residue)\n",
    "                lng = len(residues)\n",
    "                dist_matrix = np.zeros((lng, lng), np.float)\n",
    "                for i in range(0, lng):\n",
    "                    for j in range(i, lng):\n",
    "                        try:\n",
    "                            diff_vector  = residues[i]['C'].coord - residues[j]['C'].coord\n",
    "                        except Exception:\n",
    "                            # This just gets the average value around the unknown residue distance\n",
    "                            count = 0\n",
    "                            sums = 0\n",
    "                            if j - 1 > 0:\n",
    "                                count += 1\n",
    "                                sums += dist_matrix[i][j - 1]\n",
    "                            if i - 1 > 0:\n",
    "                                count += 1\n",
    "                                sums += dist_matrix[i - 1][j]\n",
    "                            if count > 0:\n",
    "                                diff_vector = sums / count\n",
    "                            else:\n",
    "                                diff_vector = 0\n",
    "                        dist = np.sqrt(np.sum(diff_vector * diff_vector))\n",
    "                        dist_matrix[i][j] = dist\n",
    "                        dist_matrix[j][i] = dist\n",
    "                    chain_dist_matrix = dist_matrix\n",
    "        \n",
    "    if partial:\n",
    "        start_pos, end_pos, bool_arr = get_seq_pos(full_pdb_id)\n",
    "        if start_pos == -1:\n",
    "            return pd.DataFrame(chain_dist_matrix[bool_arr]).iloc[:, bool_arr]\n",
    "        return pd.DataFrame(chain_dist_matrix[start_pos:end_pos]).iloc[:, start_pos:end_pos]\n",
    "    else:\n",
    "        return chain_dist_matrix\n",
    "    \n",
    "    \n",
    "# gets alignment of given sequence on top of the original sequence\n",
    "# from a fasta file downloaded from the cloud repository\n",
    "def get_seq_alignment(full_pdb_id):\n",
    "    seq = ''\n",
    "    for record in SeqIO.parse(os.path.join(fasta_path, f'{full_pdb_id}.fasta'), \"fasta\"):\n",
    "        seq = record.seq\n",
    "    pdb_id = full_pdb_id[0:4].upper()\n",
    "    #print(pdb_id)\n",
    "    get_pdb_file(pdb_id)\n",
    "    struct = get_pdb_structure(pdb_id)\n",
    "    ppb = PPBuilder()\n",
    "    for model in struct:\n",
    "        for chain in model:\n",
    "            if chain.id == full_pdb_id[4]:\n",
    "                ground_seq = ''\n",
    "                for pp in ppb.build_peptides(chain, aa_only=False):\n",
    "                    ground_seq += pp.get_sequence()\n",
    "                alignments = pairwise2.align.globalxx(ground_seq, str(seq))\n",
    "                al_seq = alignments[0][1]\n",
    "                al_seq_bool = [False if a is '-' else True for a in al_seq]\n",
    "                return al_seq_bool\n",
    "\n",
    "            \n",
    "def get_seq_pos(full_pdb_id):\n",
    "    seq = ''\n",
    "    for record in SeqIO.parse(os.path.join(fasta_path, f'{full_pdb_id}.fasta'), \"fasta\"):\n",
    "        seq = str(record.seq)\n",
    "    pdb_id = full_pdb_id[0:4].upper()\n",
    "    get_pdb_file(pdb_id)\n",
    "    struct = get_pdb_structure(pdb_id)\n",
    "    ppb = PPBuilder()\n",
    "    for model in struct:\n",
    "        for chain in model:\n",
    "            if chain.id == full_pdb_id[4]:\n",
    "                ground_seq = ''\n",
    "                for pp in ppb.build_peptides(chain, aa_only=False):\n",
    "                    ground_seq += pp.get_sequence()\n",
    "                start_pos = ground_seq.find(seq)\n",
    "                if start_pos == -1:\n",
    "                    alignments = pairwise2.align.globalxx(ground_seq, seq)\n",
    "                    al_seq = alignments[0][1]\n",
    "                    bool_seq = [False if a == '-' else True for a in al_seq]\n",
    "                    #start_pos = largest_substring_pos(ground_seq, seq)\n",
    "                    #start_pos = int(alignments[0][2] - 1)\n",
    "                    #end_pos = start_pos + len(al_seq)\n",
    "                    return (-1, -1, bool_seq)\n",
    "                else:\n",
    "                    end_pos = start_pos + len(seq)\n",
    "                    return (start_pos, end_pos, '')\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "# calls dssp api to get dssp file\n",
    "def pdb_id_to_dssp_file(pdb_id, replace=False):\n",
    "    pdb_id = pdb_id.upper()\n",
    "    rest_url = 'http://www.cmbi.umcn.nl/xssp/'\n",
    "    # Read the pdb id data into a variable\n",
    "    data = {'data': pdb_id}\n",
    "\n",
    "    # Send a request to the server to create hssp data from the pdb file data.\n",
    "    # If an error occurs, an exception is raised and the program exits. If the\n",
    "    # request is successful, the id of the job running on the server is\n",
    "    # returned.\n",
    "    url_create = f'{rest_url}api/create/pdb_id/dssp/'\n",
    "    r = requests.post(url_create, data=data)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    job_id = json.loads(r.text)['id']\n",
    "    #print(f'Job submitted successfully. Id is: {job_id}')\n",
    "\n",
    "    # Loop until the job running on the server has finished, either successfully\n",
    "    # or due to an error.\n",
    "    ready = False\n",
    "    while not ready:\n",
    "        # Check the status of the running job. If an error occurs an exception\n",
    "        # is raised and the program exits. If the request is successful, the\n",
    "        # status is returned.\n",
    "        url_status = f'{rest_url}api/status/pdb_id/dssp/{job_id}/'\n",
    "        r = requests.get(url_status)\n",
    "        r.raise_for_status()\n",
    "\n",
    "        status = json.loads(r.text)['status']\n",
    "        #print(f'Job status is: {status}')\n",
    "\n",
    "        # If the status equals SUCCESS, exit out of the loop by changing the\n",
    "        # condition ready. This causes the code to drop into the `else` block\n",
    "        # below.\n",
    "        #\n",
    "        # If the status equals either FAILURE or REVOKED, an exception is raised\n",
    "        # containing the error message. The program exits.\n",
    "        #\n",
    "        # Otherwise, wait for five seconds and start at the beginning of the\n",
    "        # loop again.\n",
    "        if status == 'SUCCESS':\n",
    "            ready = True\n",
    "        elif status in ['FAILURE', 'REVOKED']:\n",
    "            raise Exception(json.loads(r.text)['message'])\n",
    "        else:\n",
    "            time.sleep(5)\n",
    "    else:\n",
    "        # Requests the result of the job. If an error occurs an exception is\n",
    "        # raised and the program exits. If the request is successful, the result\n",
    "        # is returned.\n",
    "        url_result = f'{rest_url}api/result/pdb_id/dssp/{job_id}/'\n",
    "        r = requests.get(url_result)\n",
    "        r.raise_for_status()\n",
    "        result = json.loads(r.text)['result']\n",
    "        try:\n",
    "            file_path = os.path.join(dssp_path, f'{pdb_id}.dssp')\n",
    "            if os.path.isfile(file_path) and not replace:\n",
    "                return True\n",
    "            #print(file_path)\n",
    "            file = open(file_path, \"w\")\n",
    "            file.write(result)\n",
    "            file.close()\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "            return False\n",
    "        # Return the result to the caller, which prints it to the screen.\n",
    "        return True\n",
    "\n",
    "# uses api call file to generate secondary structure, phi/psi, solvent-accessibility stuff\n",
    "# because of indexing stuff, it crashes if we use partial = True\n",
    "# ask aaron about that\n",
    "def get_ground_truth_api(full_pdb_id, partial=False):\n",
    "    pdb_id = full_pdb_id[0:4].upper()\n",
    "    get_pdb_file(pdb_id)\n",
    "    struct = get_pdb_structure(pdb_id)\n",
    "    ppb = PPBuilder()\n",
    "    struct_info = []\n",
    "    for model in struct:\n",
    "        file = os.path.join(pdb_path, f'{pdb_id}.pdb')\n",
    "        pdb_id_to_dssp_file(pdb_id)\n",
    "        dssp = DSSP(model=model, in_file=os.path.join(dssp_path, f'{pdb_id}.dssp'), file_type='DSSP')\n",
    "        seq_count = 0\n",
    "        #dssp = DSSP(model=model, in_file=file)\n",
    "        for chain in model:\n",
    "            if chain.id == full_pdb_id[4] or not partial:\n",
    "                residues = []\n",
    "                seq = ''\n",
    "                for pp in ppb.build_peptides(chain, aa_only=False):\n",
    "                    seq += pp.get_sequence()\n",
    "                    for residue in pp:\n",
    "                        residues.append(residue)\n",
    "                for i, residue in enumerate(residues):\n",
    "                    try:\n",
    "                        key = list(dssp.keys())[seq_count]   \n",
    "                        dssp_info = dssp[key]\n",
    "                        amino_acid = dssp_info[1]\n",
    "                        sec_struct = dssp_info[2]\n",
    "                        solv_acc = dssp_info[3]\n",
    "                        phi = dssp_info[4]\n",
    "                        psi = dssp_info[5]\n",
    "                    except Exception:\n",
    "                        # DSSP didn't capture this amino acid\n",
    "                        #print(full_pdb_id, seq_count, len(dssp), len(seq), residue)\n",
    "                        amino_acid = seq[i]\n",
    "                        sec_struct = '-'\n",
    "                        solv_acc = 0\n",
    "                        phi = 360\n",
    "                        psi = 360\n",
    "                    seq_count += 1\n",
    "                    # Keys 6 through 13 is bonding energy / relidx (no clue what this is)\n",
    "                    struct_info.append([model.full_id[0], model.full_id[1], chain.id,\n",
    "                        residue.resname, residue.id[1], amino_acid, sec_struct, solv_acc, phi, psi])\n",
    "    info_df = pd.DataFrame(struct_info, \n",
    "        columns=['Model_Name', 'Model_ID', 'Chain', 'Residue_Name',\n",
    "        'Residue_ID', 'Amino_Acid', 'Secondary_Structure', 'Solvent_Accessability', \n",
    "        'Phi', 'Psi'])\n",
    "    if partial:    \n",
    "        start_pos, end_pos, bool_arr = get_seq_pos(full_pdb_id)\n",
    "        if start_pos == -1:\n",
    "            return info_df[bool_arr]\n",
    "        return info_df[start_pos:end_pos]\n",
    "    else:\n",
    "        return info_df\n",
    "\n",
    "\n",
    "    \n",
    "# the following two functions download the files from the api\n",
    "# to create a cache on the disk\n",
    "# speeds things up immensely, only need to run once though\n",
    "def get_ground_truth_files(full_pdb_id, replace=False, partial=False):\n",
    "    bad_list = ['4osnA0']\n",
    "    if full_pdb_id in bad_list:\n",
    "        return 'Bad File'\n",
    "    if partial:\n",
    "        file_path = os.path.join(protein_path, full_pdb_id + '.csv')\n",
    "    else:\n",
    "        file_path = os.path.join(protein_path, full_pdb_id[0:4] + '.csv')\n",
    "        \n",
    "    if os.path.isfile(file_path) and not replace:\n",
    "        return True\n",
    "    \n",
    "    df = get_ground_truth(full_pdb_id, partial)\n",
    "    file_path = ''\n",
    "    try:\n",
    "        df.to_csv(file_path)\n",
    "    except Exception:\n",
    "        return False\n",
    "    return True\n",
    "    \n",
    "def get_cb_distance_files(full_pdb_id, replace=False, partial=False):\n",
    "    bad_list = ['4osnA0']\n",
    "    if full_pdb_id in bad_list:\n",
    "        return 'Bad File'\n",
    "    file_path = ''\n",
    "    if partial:\n",
    "        file_path = os.path.join(distances_path, full_pdb_id + '.csv')\n",
    "        if os.path.isfile(file_path) and not replace:\n",
    "            return True\n",
    "        try:\n",
    "            df = get_cb_distances(full_pdb_id, partial)\n",
    "            df.to_csv(file_path)\n",
    "        except Exception:\n",
    "            return False\n",
    "        return True\n",
    "    else:\n",
    "        df = get_cb_distances(full_pdb_id, partial)\n",
    "        keys = df.keys\n",
    "        for key in keys:\n",
    "            file_path = os.path.join(distances_path, full_pdb_id[0:4] + key + '0.csv')\n",
    "            if os.path.isfile(file_path) and not replace:\n",
    "                return True\n",
    "            try:\n",
    "                df[key].to_csv(file_path)\n",
    "            except Exception:\n",
    "                return False\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_25=pandas.read_csv(\"datasets/protein_all_25_12_no_na.csv\")\n",
    "dataset_10=pandas.read_csv(\"datasets/protein_all_10_5_no_na.csv\")\n",
    "dataset_5=pandas.read_csv(\"datasets/protein_all_5_3_no_na.csv\")\n",
    "\n",
    "fix_chip_start_stop(dataset_25,12,25)\n",
    "fix_chip_start_stop(dataset_10,5,10)\n",
    "fix_chip_start_stop(dataset_5,3,5)\n",
    "\n",
    "\n",
    "pdb_list, length_dict, aa_seq, input_features = np.load(\"datasets/testset-input-features.npy\",allow_pickle=True)\n",
    "pdb_list_y, distance_maps_cb = np.load(\"datasets/testset-distance-maps-cb.npy\",encoding=\"latin1\",allow_pickle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def get_rows_from_segmentation(seg, dataset):\n",
    "    with open(seg_dir+seg) as jsonf:\n",
    "        a=json.load(jsonf)\n",
    "        df=dataset.iloc[[int(m) for m in a['rows']],:]\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def draw_ss_predicts(index,ss):\n",
    "    \n",
    "#     H, G and I to H; E to E; the rest to C\n",
    "    ss_dict={\n",
    "        \"H\":0,\n",
    "        \"G\":0,\n",
    "        \"I\":0,\n",
    "        \"E\":1,\n",
    "        \"B\":2,\n",
    "        \"T\":2,\n",
    "        \"S\":2             \n",
    "            }\n",
    "    \n",
    "    aa_length=length_dict[pdb_list[index]]\n",
    "    \n",
    "    plt.figure(figsize=(15,10))\n",
    "\n",
    "    df25=dataset_25[dataset_25[\"protein_id\"]==dataset_25[\"protein_id\"].unique()[index]] \n",
    "    df10=dataset_10[dataset_10[\"protein_id\"]==dataset_10[\"protein_id\"].unique()[index]] \n",
    "    df5=dataset_5[dataset_5[\"protein_id\"]==dataset_5[\"protein_id\"].unique()[index]] \n",
    "\n",
    "    plt.subplot(4,1,1)\n",
    "    y=ground_truth_window_avg(df5)[\"ground_truth_\"+ss]\n",
    "    x=range(0,len(y))\n",
    "    plt.bar(x,y,1)\n",
    "    plt.xlim((0,aa_length))    \n",
    "\n",
    "    plt.subplot(4,1,2)\n",
    "    y=ground_truth_window_avg(df10)[\"ground_truth_\"+ss]\n",
    "    x=range(0,len(y))\n",
    "    plt.bar(x,y,1)\n",
    "    plt.xlim((0,aa_length))\n",
    "\n",
    "    plt.subplot(4,1,3)\n",
    "    y=ground_truth_window_avg(df25)[\"ground_truth_\"+ss]\n",
    "    x=range(0,len(y))\n",
    "    plt.bar(x,y,1)\n",
    "    plt.xlim((0,aa_length))\n",
    "\n",
    "    plt.subplot(4,1,4)\n",
    "    y=[1-n for n in input_features[pdb_list[index]][ss_dict[ss]].astype(np.float)]\n",
    "    x=range(0,len(y))\n",
    "    plt.bar(x,y,1)\n",
    "    plt.xlim((0,aa_length))\n",
    "\n",
    "    \n",
    "    \n",
    "def get_inv_psipred_from_ss(protein):\n",
    "#     H to H; E to E; the rest to C\n",
    "    ss_dict={\n",
    "        \"H\":0,\n",
    "        \"G\":0,\n",
    "        \"I\":0,\n",
    "        \"E\":1,\n",
    "        \"B\":1,\n",
    "        \"T\":2,\n",
    "        \"S\":2,\n",
    "        \"-\":2\n",
    "            }\n",
    "    \n",
    "    # I subtract it from 1 to make it on the same scale as the other predictions\n",
    "    # (i.e. high means more likely to be X)\n",
    "    \n",
    "    return [1-n for n in input_features[protein][ss_dict[ss]].astype(np.float)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"HBEGITS\"\n",
    "# draw_ss_predicts(1,\"H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reduced_ss(protein_id):\n",
    "    ss_file=os.path.join(data_path,\"ss\",protein_id+\".ss\")\n",
    "    sf=open(ss_file).read()\n",
    "    return(list(sf.split(\"\\n\")[1]))\n",
    "   \n",
    "# get_reduced_ss(\"1bebA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth_window_avg(df):\n",
    "    aa_length=max(df[\"stop\"])\n",
    "    mean_ss=[]\n",
    "    gt=[\"ground_truth_\"+ss for ss in \"HBEGITS\"]\n",
    "\n",
    "    for aa in range(0,aa_length):\n",
    "        chip=df[(df[\"start\"]<=aa) & (df[\"stop\"]>aa)]\n",
    "        mean_ss.append(chip[gt].mean(0))\n",
    "    gt_avg=pandas.DataFrame(mean_ss)\n",
    "    \n",
    "    return gt_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each protein:\n",
    "#     for chip in protein\n",
    "#         get avg ss's for chip\n",
    "#     get avg ss for each aa\n",
    "#     get psipred for each aa\n",
    "#     present as bar graph\n",
    "\n",
    "#     get gt for protein !!!!\n",
    "\n",
    "#     get thd_rmse for protein\n",
    "#     get psipred_rmse for protein\n",
    "#     put rmse's in vectors\n",
    "\n",
    "# graph rmse's\n",
    "\n",
    "# the following is pseudocode\n",
    "\n",
    "def rmse(a,b):\n",
    "    return np.sqrt(np.mean((a-b)**2))\n",
    "\n",
    "def eval_test_data_new(leaf_df,dataset):\n",
    "    output_df=pandas.DataFrame()\n",
    "    \n",
    "    for protein in pandas.unique(leaf_df['protein_id']):\n",
    "        print(protein)\n",
    "        mean_ss=[]\n",
    "        gt_names=[\"pred_gt_\"+ss for ss in \"HBEGITS\"]\n",
    "\n",
    "        # this part is avg'ing across the wrong stuff\n",
    "        # all the values end up the same\n",
    "        \n",
    "        for aa in range(length_dict[protein]):\n",
    "            chip=leaf_df[(leaf_df[\"start\"]<=aa) & (leaf_df[\"stop\"]>aa)]\n",
    "            mean_ss.append([protein]+list(chip[gt_names].mean(0)))\n",
    "        gt_avg=pandas.DataFrame(mean_ss)\n",
    "        gt_avg.columns=[\"protein_id\"]+[\"pred_ss_\"+ss for ss in \"HBEGITS\"]\n",
    "        \n",
    "        \n",
    "        ssgt=[]\n",
    "        \n",
    "        for sec_s in \"HBEGITS\":\n",
    "        # get gt for protein\n",
    "            ss=get_ground_truth_api(protein)['Secondary_Structure']\n",
    "            gt=[int(x) for x in ss==sec_s]\n",
    "            gt_avg[\"ground_truth_\"+sec_s]=gt\n",
    "            \n",
    "        gt_avg['psipred_helix']=input_features[protein][0].astype(np.float)\n",
    "        gt_avg['psipred_sheet']=input_features[protein][1].astype(np.float)\n",
    "        gt_avg['psipred_coil']=input_features[protein][2].astype(np.float)\n",
    "        \n",
    "        output_df.append(gt_avg)\n",
    "    return output_df\n",
    "\n",
    "    \n",
    "def eval_test_data_reduced(leaf_df,dataset):\n",
    "    output_df=pandas.DataFrame()\n",
    "    \n",
    "    for protein in pandas.unique(leaf_df['protein_id']):\n",
    "        \n",
    "        df_prot=leaf_df[leaf_df['protein_id']==protein]\n",
    "        \n",
    "        print(protein)\n",
    "        mean_ss=[]\n",
    "        gt_names=[\"thd_pred_helix\",\"thd_pred_sheet\",\"thd_pred_coil\"]\n",
    "\n",
    "        # this part is avg'ing across the wrong stuff\n",
    "        # all the values end up the same\n",
    "        \n",
    "        for aa in range(length_dict[protein]):\n",
    "            chip=df_prot[(df_prot[\"start\"]<=aa) & (df_prot[\"stop\"]>aa)]\n",
    "            mean_ss.append([protein,aa]+list(chip[gt_names].mean(0)))\n",
    "        gt_avg=pandas.DataFrame(mean_ss)\n",
    "        gt_avg.columns=[\"protein_id\",\"aa_index\"]+[\"pred_ss_\"+ss for ss in \"HEC\"]\n",
    "        \n",
    "        \n",
    "        ssgt=[]\n",
    "        \n",
    "        for sec_s in \"HEC\":\n",
    "        # get gt for protein\n",
    "            ss=get_reduced_ss(protein)\n",
    "            gt=[int(x==sec_s) for x in ss]\n",
    "            gt_avg[\"ground_truth_\"+sec_s]=gt\n",
    "            \n",
    "        gt_avg['psipred_helix']=input_features[protein][0].astype(np.float)\n",
    "        gt_avg['psipred_sheet']=input_features[protein][1].astype(np.float)\n",
    "        gt_avg['psipred_coil']=input_features[protein][2].astype(np.float)\n",
    "        \n",
    "        gt_avg['inv_psipred_helix']=1-input_features[protein][0].astype(np.float)\n",
    "        gt_avg['inv_psipred_sheet']=1-input_features[protein][1].astype(np.float)\n",
    "        gt_avg['inv_psipred_coil']=1-input_features[protein][2].astype(np.float)\n",
    " \n",
    "        \n",
    "        output_df=output_df.append(gt_avg)\n",
    "        \n",
    "    return output_df\n",
    "\n",
    "    \n",
    "    \n",
    "def seg_rows_formatted(thd_name,file_id,dataset):\n",
    "    seg=\"GROUPDATA_\"+thd_name+\" \"+file_id+\".json\"\n",
    "    return get_rows_from_segmentation(thd_name+\"/\"+seg,dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pred_gt(leafdf,dataset):\n",
    "\n",
    "    gt=[\"ground_truth_\"+ss for ss in \"HBEGITS\"]\n",
    "\n",
    "    ss_pred=pandas.DataFrame()\n",
    "\n",
    "    for index, row in leafdf.iterrows():\n",
    "        ssp=seg_rows_formatted(row['thd_id'],row['file_id'],dataset)[gt].mean(0)\n",
    "        ss_pred=ss_pred.append(ssp, ignore_index=True)\n",
    "        \n",
    "    ss_pred.columns=[\"pred_gt_\"+ss for ss in \"HBEGITS\"]\n",
    "    return pandas.concat([leafdf,ss_pred],axis=1)\n",
    "\n",
    "def add_pred_condensed(leafdf,dataset):\n",
    "    \n",
    "    gt=[\"ground_truth_\"+ss for ss in \"HBEGITS\"]\n",
    "\n",
    "    ss_pred=pandas.DataFrame()\n",
    "\n",
    "    for index, row in leafdf.iterrows():\n",
    "        ssp=seg_rows_formatted(row['thd_id'],row['file_id'],dataset)[gt]\n",
    "        \n",
    "        helix=[\"ground_truth_\"+ss for ss in \"HGI\"]\n",
    "        sheet=[\"ground_truth_\"+ss for ss in \"EB\"]\n",
    "        coil=[\"ground_truth_\"+ss for ss in \"TS\"]\n",
    "\n",
    "        hlx=ssp[helix].sum(axis=1)\n",
    "        sh=ssp[sheet].sum(axis=1)\n",
    "        cl=1-(hlx+sh)\n",
    "\n",
    "        pred=pandas.concat([hlx,sh,cl],axis=1)\n",
    "\n",
    "        ss_pred=ss_pred.append(pred.mean(axis=0),ignore_index=True)\n",
    "    ss_pred.columns=[\"thd_pred_helix\",\"thd_pred_sheet\",\"thd_pred_coil\"]\n",
    "    return pandas.concat([leafdf,ss_pred],axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "thd_leafs=add_pred_condensed(pandas.read_csv(\"datasets/test6_sequences_pred.csv\"),dataset_5)\n",
    "thd_leafs.to_csv(\"data/thd_leafs.csv\")\n",
    "\n",
    "prediction_df=eval_test_data_reduced(thd_leafs,dataset_5)\n",
    "prediction_df.to_csv(\"data/reduced_predict_fixed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this gives us a table of the accuracy results\n",
    "\n",
    "def pred_accuracy(pred_df):\n",
    "\n",
    "    rmse_df=pandas.DataFrame()\n",
    "\n",
    "    psipred=[\"inv_psipred_helix\",\"inv_psipred_sheet\",\"inv_psipred_coil\"]\n",
    "\n",
    "    results_df=pandas.DataFrame()\n",
    "\n",
    "    for ss in range(3):\n",
    "\n",
    "        pred=\"pred_ss_\"+\"HEC\"[ss]\n",
    "        gt=\"ground_truth_\"+\"HEC\"[ss]\n",
    "        psi=psipred[ss]\n",
    "\n",
    "        rmse_pred=rmse(pred_df[pred],pred_df[gt])\n",
    "        acc_pred=1-np.mean(abs(pred_df[pred]-pred_df[gt]))\n",
    "\n",
    "        rmse_psi=rmse(pred_df[psi],pred_df[gt])\n",
    "        acc_psi=1-np.mean(abs(pred_df[psi]-pred_df[gt]))\n",
    "\n",
    "        row=[[\"helix\",\"sheet\",\"coil\"][ss]]+[rmse_pred,acc_pred,rmse_psi,acc_psi]\n",
    "        row=pandas.Series(row)\n",
    "        results_df=results_df.append(row,ignore_index=True)\n",
    "\n",
    "    columns=[\"structure\"]\n",
    "    columns+=[\"thd_pred_rmse\",\"thd_pred_accuracy\"]\n",
    "    columns+=[\"psipred_rmse\",\"psipred_accuracy\"]\n",
    "\n",
    "    results_df.columns=columns\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "    \n",
    "# see sequence graphed on all classes\n",
    "#      because ryan wants to know how to threshhold it\n",
    "# accuracy for the aa that are helices\n",
    "# add a none % column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# accuracy for the aa that ARE x\n",
    "\n",
    "def pred_accuracy_signal(pred_df):\n",
    "\n",
    "    rmse_df=pandas.DataFrame()\n",
    "\n",
    "    psipred=[\"inv_psipred_helix\",\"inv_psipred_sheet\",\"inv_psipred_coil\"]\n",
    "\n",
    "    results_df=pandas.DataFrame()\n",
    "\n",
    "    for ss in range(3):\n",
    "\n",
    "        pdf2=pred_df[pred_df['ground_truth_'+\"HEC\"[ss]]==1]\n",
    "\n",
    "        pred=\"pred_ss_\"+\"HEC\"[ss]\n",
    "        gt=\"ground_truth_\"+\"HEC\"[ss]\n",
    "        psi=psipred[ss]\n",
    "\n",
    "        rmse_pred=rmse(pdf2[pred],pdf2[gt])\n",
    "        acc_pred=1-np.mean(abs(pdf2[pred]-pdf2[gt]))\n",
    "\n",
    "        rmse_psi=rmse(pdf2[psi],pdf2[gt])\n",
    "        acc_psi=1-np.mean(abs(pdf2[psi]-pdf2[gt]))\n",
    "\n",
    "        row=[[\"helix\",\"sheet\",\"coil\"][ss]]+[rmse_pred,acc_pred,rmse_psi,acc_psi]\n",
    "        row=pandas.Series(row)\n",
    "        results_df=results_df.append(row,ignore_index=True)\n",
    "\n",
    "    columns=[\"structure\"]\n",
    "    columns+=[\"thd_pred_rmse\",\"thd_pred_accuracy\"]\n",
    "    columns+=[\"psipred_rmse\",\"psipred_accuracy\"]\n",
    "\n",
    "    results_df.columns=columns\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_strat = pandas.read_csv(os.path.join(data_path,\"Cor_distance_AllofSourceData_NHLOne_NHLTwo_test6_seq_pred.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test=pandas.DataFrame()\n",
    "\n",
    "for strat in range(7):\n",
    "    print(\"testing strat \"+str(strat))\n",
    "    \n",
    "    cur_strat=m_strat.loc[0:5]\n",
    "    cur_strat[\"file_id\"]=cur_strat[\"file_id_\"+str(strat)]\n",
    "    \n",
    "    cur_strat=add_pred_condensed(m_strat,dataset_5)\n",
    "    \n",
    "    pred_df=eval_test_data_reduced(cur_strat,dataset_5)\n",
    "    \n",
    "    strat_name=pandas.Series(strat,name=\"strat\")\n",
    "    \n",
    "    pa=pred_accuracy(pred_df)\n",
    "    \n",
    "    pas=pred_accuracy_signal(pred_df)\n",
    "    \n",
    "    strat_result=pandas.concat(strat_name,pa,pas)\n",
    "    \n",
    "    strat_test.append(strat_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
